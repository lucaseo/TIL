{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Basics\n",
    "\n",
    "$$ y = ax + \\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Aim of Linear Regression\n",
    "- Minimize the distance between the ponts and the line ( $y = ax + \\beta$ )\n",
    "- By adjusting\n",
    "    - Coefficient ( $a$ )\n",
    "    - Bias/Intercept ( $\\beta$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building a Linear Regression Model\n",
    "\n",
    "$$ y = 2x + 1 $$\n",
    "\n",
    "- Coeffeicent $a = 2$\n",
    "- Bias / Intercept $\\beta = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 1. Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example dataset\n",
    "\n",
    "x_values = [i for i in range(11)]\n",
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "# convert to numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need 2 dimensional array\n",
    "\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values = [2*i +1 for i in x_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 2. Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # required for linear regression model\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Model Equation\n",
    "    - $y = 2x + 1$\n",
    "    \n",
    "2. Forward\n",
    "    - Example\n",
    "        - Input $x = 1$\n",
    "        - Output $\\hat{y}  = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 3. Create Linear Regression Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    \n",
    "    # initialize\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        # python super function\n",
    "        # inherite all from nn.Module\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        \n",
    "        # pass input, output dimension \n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    # for every value of x, goes through linear model\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 4. Create Loss Class\n",
    "\n",
    "- MSE Loss : Mean Squared Error\n",
    "- $MSE = \\dfrac{1}{n} \\sum_{i=1}^{n}(\\hat{y}_i - y_i$\n",
    "    - $\\hat{y}$ : prediction\n",
    "    - $y$ : true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 5 Optimizer Class\n",
    "\n",
    "Simplified Equation\n",
    "- $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta$\n",
    "- ( `parameters` = `parameters` - `learning_rate` x `parameters_gradient` )\n",
    "    - $\\theta$ : parameters(our variables)\n",
    "    - $\\eta$ : learning rate(how fast we want to learn\n",
    "    - $\\nabla_\\theta$ : parameters' gradients\n",
    "\n",
    "    \n",
    "- Parameters in our linear regression model\n",
    "    - $a$ , $\\beta$ in $y = ax + \\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "# Steepest Gradient Decent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 6 Train Model\n",
    "\n",
    "- 1 epoch : going through the whole `x_train` data once\n",
    "    - 100 epoch:\n",
    "        - 100 x mapping `x_train`\n",
    "        \n",
    "- Process steps\n",
    "    1. convert inputs/labels to variables\n",
    "    2. clear gradent buffets\n",
    "    3. get output given inputs\n",
    "    4. get loss\n",
    "    5. get gradient w.r.t. parameters\n",
    "    6. update parameters using gradents\n",
    "        - `parameters` = `parameters` - `learning_rate` x `parameters_gradient`\n",
    "    7. Repeat Whole Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss158.53024291992188\n",
      "epoch 2, loss13.26687240600586\n",
      "epoch 3, loss1.414445400238037\n",
      "epoch 4, loss0.44397029280662537\n",
      "epoch 5, loss0.36114197969436646\n",
      "epoch 6, loss0.3507571220397949\n",
      "epoch 7, loss0.34632259607315063\n",
      "epoch 8, loss0.3424127697944641\n",
      "epoch 9, loss0.33858588337898254\n",
      "epoch 10, loss0.33480456471443176\n",
      "epoch 11, loss0.33106568455696106\n",
      "epoch 12, loss0.3273687958717346\n",
      "epoch 13, loss0.3237132132053375\n",
      "epoch 14, loss0.3200986087322235\n",
      "epoch 15, loss0.3165237009525299\n",
      "epoch 16, loss0.31298932433128357\n",
      "epoch 17, loss0.30949410796165466\n",
      "epoch 18, loss0.3060379922389984\n",
      "epoch 19, loss0.3026205599308014\n",
      "epoch 20, loss0.299241304397583\n",
      "epoch 21, loss0.29589974880218506\n",
      "epoch 22, loss0.2925955057144165\n",
      "epoch 23, loss0.28932827711105347\n",
      "epoch 24, loss0.28609699010849\n",
      "epoch 25, loss0.28290247917175293\n",
      "epoch 26, loss0.2797434329986572\n",
      "epoch 27, loss0.27661949396133423\n",
      "epoch 28, loss0.273530513048172\n",
      "epoch 29, loss0.2704758942127228\n",
      "epoch 30, loss0.2674556076526642\n",
      "epoch 31, loss0.2644689381122589\n",
      "epoch 32, loss0.2615155875682831\n",
      "epoch 33, loss0.2585952877998352\n",
      "epoch 34, loss0.2557077407836914\n",
      "epoch 35, loss0.2528523802757263\n",
      "epoch 36, loss0.2500286102294922\n",
      "epoch 37, loss0.24723675847053528\n",
      "epoch 38, loss0.24447602033615112\n",
      "epoch 39, loss0.24174578487873077\n",
      "epoch 40, loss0.23904626071453094\n",
      "epoch 41, loss0.23637697100639343\n",
      "epoch 42, loss0.23373737931251526\n",
      "epoch 43, loss0.23112693428993225\n",
      "epoch 44, loss0.2285463511943817\n",
      "epoch 45, loss0.22599412500858307\n",
      "epoch 46, loss0.22347024083137512\n",
      "epoch 47, loss0.22097477316856384\n",
      "epoch 48, loss0.21850737929344177\n",
      "epoch 49, loss0.21606749296188354\n",
      "epoch 50, loss0.21365433931350708\n",
      "epoch 51, loss0.2112688273191452\n",
      "epoch 52, loss0.2089095115661621\n",
      "epoch 53, loss0.20657669007778168\n",
      "epoch 54, loss0.2042699009180069\n",
      "epoch 55, loss0.20198866724967957\n",
      "epoch 56, loss0.19973331689834595\n",
      "epoch 57, loss0.19750289618968964\n",
      "epoch 58, loss0.19529730081558228\n",
      "epoch 59, loss0.1931164413690567\n",
      "epoch 60, loss0.1909598708152771\n",
      "epoch 61, loss0.1888274848461151\n",
      "epoch 62, loss0.18671880662441254\n",
      "epoch 63, loss0.18463386595249176\n",
      "epoch 64, loss0.18257224559783936\n",
      "epoch 65, loss0.18053334951400757\n",
      "epoch 66, loss0.17851725220680237\n",
      "epoch 67, loss0.1765240728855133\n",
      "epoch 68, loss0.17455270886421204\n",
      "epoch 69, loss0.17260347306728363\n",
      "epoch 70, loss0.1706760972738266\n",
      "epoch 71, loss0.16877013444900513\n",
      "epoch 72, loss0.16688549518585205\n",
      "epoch 73, loss0.16502206027507782\n",
      "epoch 74, loss0.16317901015281677\n",
      "epoch 75, loss0.16135694086551666\n",
      "epoch 76, loss0.15955522656440735\n",
      "epoch 77, loss0.15777334570884705\n",
      "epoch 78, loss0.1560114622116089\n",
      "epoch 79, loss0.15426918864250183\n",
      "epoch 80, loss0.15254658460617065\n",
      "epoch 81, loss0.15084317326545715\n",
      "epoch 82, loss0.14915873110294342\n",
      "epoch 83, loss0.1474931836128235\n",
      "epoch 84, loss0.1458461731672287\n",
      "epoch 85, loss0.14421746134757996\n",
      "epoch 86, loss0.1426069587469101\n",
      "epoch 87, loss0.14101451635360718\n",
      "epoch 88, loss0.13943980634212494\n",
      "epoch 89, loss0.13788273930549622\n",
      "epoch 90, loss0.13634298741817474\n",
      "epoch 91, loss0.1348205804824829\n",
      "epoch 92, loss0.13331489264965057\n",
      "epoch 93, loss0.1318262666463852\n",
      "epoch 94, loss0.1303541362285614\n",
      "epoch 95, loss0.1288984715938568\n",
      "epoch 96, loss0.12745918333530426\n",
      "epoch 97, loss0.12603594362735748\n",
      "epoch 98, loss0.12462838739156723\n",
      "epoch 99, loss0.12323673814535141\n",
      "epoch 100, loss0.12186045944690704\n",
      "epoch 101, loss0.12049966305494308\n",
      "epoch 102, loss0.11915422230958939\n",
      "epoch 103, loss0.11782349646091461\n",
      "epoch 104, loss0.1165078803896904\n",
      "epoch 105, loss0.11520689725875854\n",
      "epoch 106, loss0.11392047256231308\n",
      "epoch 107, loss0.11264820396900177\n",
      "epoch 108, loss0.11139025539159775\n",
      "epoch 109, loss0.11014633625745773\n",
      "epoch 110, loss0.10891634970903397\n",
      "epoch 111, loss0.10770028829574585\n",
      "epoch 112, loss0.10649742186069489\n",
      "epoch 113, loss0.10530818998813629\n",
      "epoch 114, loss0.1041322872042656\n",
      "epoch 115, loss0.10296948254108429\n",
      "epoch 116, loss0.10181958973407745\n",
      "epoch 117, loss0.10068246722221375\n",
      "epoch 118, loss0.09955830872058868\n",
      "epoch 119, loss0.09844667464494705\n",
      "epoch 120, loss0.09734727442264557\n",
      "epoch 121, loss0.096260204911232\n",
      "epoch 122, loss0.0951852798461914\n",
      "epoch 123, loss0.09412234276533127\n",
      "epoch 124, loss0.09307127445936203\n",
      "epoch 125, loss0.09203202277421951\n",
      "epoch 126, loss0.09100428968667984\n",
      "epoch 127, loss0.08998808264732361\n",
      "epoch 128, loss0.08898311853408813\n",
      "epoch 129, loss0.087989442050457\n",
      "epoch 130, loss0.08700693398714066\n",
      "epoch 131, loss0.08603535592556\n",
      "epoch 132, loss0.08507443964481354\n",
      "epoch 133, loss0.08412452787160873\n",
      "epoch 134, loss0.0831850990653038\n",
      "epoch 135, loss0.08225620537996292\n",
      "epoch 136, loss0.08133771270513535\n",
      "epoch 137, loss0.08042943477630615\n",
      "epoch 138, loss0.0795312225818634\n",
      "epoch 139, loss0.07864310592412949\n",
      "epoch 140, loss0.07776498794555664\n",
      "epoch 141, loss0.07689647376537323\n",
      "epoch 142, loss0.07603789865970612\n",
      "epoch 143, loss0.07518875598907471\n",
      "epoch 144, loss0.07434917986392975\n",
      "epoch 145, loss0.07351883500814438\n",
      "epoch 146, loss0.07269784808158875\n",
      "epoch 147, loss0.07188618928194046\n",
      "epoch 148, loss0.07108335196971893\n",
      "epoch 149, loss0.07028958946466446\n",
      "epoch 150, loss0.06950458884239197\n",
      "epoch 151, loss0.06872858852148056\n",
      "epoch 152, loss0.06796109676361084\n",
      "epoch 153, loss0.06720203161239624\n",
      "epoch 154, loss0.06645168364048004\n",
      "epoch 155, loss0.06570976227521896\n",
      "epoch 156, loss0.06497588753700256\n",
      "epoch 157, loss0.06425030529499054\n",
      "epoch 158, loss0.06353285908699036\n",
      "epoch 159, loss0.06282339245080948\n",
      "epoch 160, loss0.06212181970477104\n",
      "epoch 161, loss0.06142815575003624\n",
      "epoch 162, loss0.06074221059679985\n",
      "epoch 163, loss0.06006396561861038\n",
      "epoch 164, loss0.05939308926463127\n",
      "epoch 165, loss0.058729954063892365\n",
      "epoch 166, loss0.05807425454258919\n",
      "epoch 167, loss0.05742558091878891\n",
      "epoch 168, loss0.056784383952617645\n",
      "epoch 169, loss0.056150272488594055\n",
      "epoch 170, loss0.05552324652671814\n",
      "epoch 171, loss0.054903268814086914\n",
      "epoch 172, loss0.05429000407457352\n",
      "epoch 173, loss0.05368386209011078\n",
      "epoch 174, loss0.0530843622982502\n",
      "epoch 175, loss0.05249166116118431\n",
      "epoch 176, loss0.05190546438097954\n",
      "epoch 177, loss0.05132579058408737\n",
      "epoch 178, loss0.05075262859463692\n",
      "epoch 179, loss0.05018594115972519\n",
      "epoch 180, loss0.04962552711367607\n",
      "epoch 181, loss0.049071408808231354\n",
      "epoch 182, loss0.04852339252829552\n",
      "epoch 183, loss0.047981537878513336\n",
      "epoch 184, loss0.04744580760598183\n",
      "epoch 185, loss0.04691598564386368\n",
      "epoch 186, loss0.04639199748635292\n",
      "epoch 187, loss0.04587390646338463\n",
      "epoch 188, loss0.0453617163002491\n",
      "epoch 189, loss0.04485514760017395\n",
      "epoch 190, loss0.044354282319545746\n",
      "epoch 191, loss0.04385886341333389\n",
      "epoch 192, loss0.0433691143989563\n",
      "epoch 193, loss0.042884860187768936\n",
      "epoch 194, loss0.04240594431757927\n",
      "epoch 195, loss0.04193245247006416\n",
      "epoch 196, loss0.041464198380708694\n",
      "epoch 197, loss0.04100121930241585\n",
      "epoch 198, loss0.0405433289706707\n",
      "epoch 199, loss0.040090497583150864\n",
      "epoch 200, loss0.03964286670088768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    \n",
    "    # convert numpy array to Torch Variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # clear gradient w.r.t. parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch {}, loss{}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 7. Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.62962395],\n",
       "       [ 2.6829612 ],\n",
       "       [ 4.7362986 ],\n",
       "       [ 6.789636  ],\n",
       "       [ 8.842974  ],\n",
       "       [10.896311  ],\n",
       "       [12.949649  ],\n",
       "       [15.002986  ],\n",
       "       [17.056322  ],\n",
       "       [19.10966   ],\n",
       "       [21.162996  ]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11375a4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "predicted = predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, predicted, '--', label='Prediction', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model is True:\n",
    "    torch.save(model.state_dict(), 'lr_model_example.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model is True:\n",
    "    model.load_state_dict(torch.load('lr_model_example.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
