{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural Network with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three basic function of neural network\n",
    "\n",
    "There may be more functions needed, but for now we will be using the following functions to make a start\n",
    "\n",
    "- Initialization : to set the number of input, hidden and output nodes\n",
    "- Train : refine the weights after being given a training set example to learn from\n",
    "- Query : give an answer from the output nodes after being given an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network class definition\n",
    "class Neural_Network():\n",
    "    \n",
    "    # initialize the neural network\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Network\n",
    "\n",
    "We need to set the number of input, hidden and output layer nodes which lead to defining the shape and size of the neural network. Let's set them by parameters of the neural network object. This way, we can easily retain the choice to create new neural networks of different sizes in other situation.\n",
    "\n",
    "We will try to develop code for a neural network which tries to keep as many useful options open, and assumptions to a minimum, so that the code can easily be used for different needs. Same class could be able to create a small network and a large one as well.\n",
    "\n",
    "Setting parameters is a way to set size, number of nodes, learning rate ... etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network class definition\n",
    "class Neural_Network():\n",
    "    \n",
    "    # initialize the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # train the neural network\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's create a small neural network object with 3 nodes in each layer and a learning rate of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of neural network\n",
    "n = Neural_Network(input_nodes, hidden_nodes, output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've told the neural network object how many input, hidden and output layer nodes we want, but nothing has really been done about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights, the Heart of the Network\n",
    "\n",
    "Next step is to create the network of nodes and links. The most important part of the network is the link weights. **Link weights** are used to calculate the signal being fed forward, the error as it's propagated backwards, and links are refined in an attempt to improve the network\n",
    "\n",
    "As in matrix, we can create:\n",
    "\n",
    "$ \\text{W}_{\\text{input hidden}} $ : (hidden_nodes by input_nodes) matrix for the weights for links between the input and hidden layers.\n",
    "\n",
    "$ \\text{W}_{\\text{hidden output}} $ : (output_nodes by hidden_nodes) matrix for the links between the hidden and output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial value of the link weights should be small and random. The following numpy function generates an array of values selected randomly betwen 0 and 1, where the size is (rows by columns)\n",
    "\n",
    "Weights can be negative not just positive in a range of -1.0 ~ +1.0 \n",
    "\n",
    "Here for simplicity, we'll just subtract 0.5 from each of the valuesto have a range between -0.5 ~ +0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05860441,  0.28547311,  0.27055259],\n",
       "       [-0.13555499,  0.28206397, -0.46915288],\n",
       "       [-0.05571964,  0.28739358,  0.3721042 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3, 3) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the two link weight matrics using the `self.inods`, `self.hnodes` and `self.onodes` to set the right size."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# link weight matrics, wih and who\n",
    "# weights inside the arrays are w_i_j, where link is from node_i to node_j in the next layer\n",
    "# w11, w21\n",
    "# w12, w22 etc\n",
    "\n",
    "self.wih = (np.random.rand(self.inodes, self.hnodes) - 0.5)\n",
    "self.who = (np.random.rand(self.hnodes, self.onodes) - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slightly more sophisticated weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights can be sampled from a normal probability distribution centered around zero and with a standard deviation that is related to the number of incoming links into a node, $1 / \\sqrt{\\text{(number of incoming links)}}$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "self.wih = (np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "self.who = (np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've set the center of the normal distribution to 0.0. The expression for the standard deviation related to the number of nodes in the next layer which is simply raising the number of nodes to the power of -0.5 (squared root , reciprocal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queryint the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`query()` function takes the input to a neural network and returns the network's ouput. We need to pass the input signals from the input layer of nodes, through the hidden layer and out of the final output layer. We also need to use the link weights to moderate the signals as they feed into any given hidden or output node, and use sigmoid activation function to squish the signal coming out of those nodes\n",
    "\n",
    "The following shows how the matix of weights for the link between the input and hidden layers can be combined with the matrix of inputs to give the signals into the hidden layer nodes:\n",
    "\n",
    "- $\\text{X}_\\text{hidden}  = \\text{W}_\\text{input_hidden} \\cdot \\text{I}$\n",
    "\n",
    "or we can use matrix multiplication approach in Numpy Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# calculate signals into hidden layer\n",
    "hidden_inputs = numpy.dot(self.wih, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the signals emerging from the hidden node, we simply apply the sigmoid squashing function to each of these emerging signals.\n",
    "\n",
    "- $\\text{O}_\\text{hidden}  = \\text{sigmoid} \\; (\\text{X}_\\text{hidden})$\n",
    "\n",
    "The scipy Python library has a set of special functions, and the sigmoid function is called `expit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines the activation function we want to use insdie the nueral network's initialization section. The lambda function takes x and returns scipy.special.expit(x) which is the sigmoid function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# activation function is the sigmoid function\n",
    "self.activation_function = lambda x: scipy.special.expit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to apply the activation function to the combined and moderated signals into the hidden nodes. The signals emerging from the hidden layer nodes are in the matrix called hidden_outputs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# calculate the signals emerging from hidden layer\n",
    "hidden_outputs = self.activation_function(hidden_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the final output layer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# calculate signals into final output layer\n",
    "final_inputs = np.dot(self.who, hidden_outputs)\n",
    "\n",
    "# calculate the signals emerging from final output layer\n",
    "final_outputs = self.activation_function(final_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "# neural network class definition\n",
    "class Neural_Network():\n",
    "    \n",
    "    # initialize the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "\n",
    "        \n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "\n",
    "        \n",
    "        # link weight matrics, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node_i to node_j in the next layer\n",
    "        # w11, w21\n",
    "        # w12, w22 etc\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "                    \n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "                    \n",
    "                    \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # train the neural network\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "                    \n",
    "                    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        \n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "           \n",
    "        #calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)          \n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "                    \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the code. The following shows the creation of small network with 3 nodes in each of the input, hidden and output layers, and queries it with a randomly chosen input of (1.0, 0.5, -1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "learning_rate = 0.3\n",
    "\n",
    "n = Neural_Network(input_nodes, hidden_nodes, output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45528079],\n",
       "       [0.47809968],\n",
       "       [0.47913207]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.query([1.0, 0.5, -1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network\n",
    "\n",
    "There are two phases of training;\n",
    "1. Calculating the output for a given training example just as `query()` does it.\n",
    "2. Taking the calculated output, comparing it with the desired output, and using the dfference to guide the updating of the network weights by backpropagating the errors to inform how the link weights are refined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for first part is almost exactly the same as that in the `query()` fucntion, because we're feeding forward the signal from the input layer to the final output layer in exactly the same way.\n",
    "\n",
    "The only difference is that we have an additional parameter, `targets_list`. You can't train the network without the training examples which include the desired or target answer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# train the neural network\n",
    "def train(self, inputs_list, targets_list):\n",
    "    #convert inputs list to 2d array\n",
    "    inputs = np.array(inputs_list, ndmin=2).T\n",
    "    targets = np.array(targets_list, ndmin=2).T\n",
    "    \n",
    "    # calculate signals into hidden layer\n",
    "    hidden_inputs = np.dot(self.wih, inputs)\n",
    "    # calculate the signals emerging from hidden layer\n",
    "    hidden_outputs = self.activation_function(hidden_inputs)\n",
    "    \n",
    "    # calculate signals into final output layer\n",
    "    final_inputs = np.dot(self.who, hidden_outputs)\n",
    "    # calculate the signals emerging from final output layers\n",
    "    final_outputs = self.activation_function(final_inputs)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to improve weights based on the error between the calculated and target output\n",
    "\n",
    "First, we need to calculate the error, which is the difference between the desired target output provided by the training example, and the actual calculated value."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# error is the (target - actual)\n",
    "output_errors = targets - final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the back-propagated errors for the hidden layer nodes. Remember how we split the errors according to the connected weights, and recombine them for each hidden layer node. We worked out the matrix form of this calculation as;\n",
    "\n",
    "$$\\text{errors}_\\text{hidden} = \\text{weights}^T_{\\text{ hidden_output}} \\cdot \\text{errors}_{\\text{ output}} $$\n",
    "\n",
    "or in Python,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "hidden_errors = np.dot(self.who.T, output_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have what we need to refine the weights at each layer. \n",
    "\n",
    "For the weights between the hidden and final layer, we use the `output_errors`. For the weights between the input and hidden layers, we use these `hidden_errors` we just calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For updating the weight for the link between a node j and a node k in the next layer in matrix form;\n",
    " \n",
    "$$ \\bigtriangleup  W_{jk} = \\alpha * E_k * \\text{sigmoid}(O_k) * (1-\\text{sigmoid}(O_k) \\cdot O^T_j$$\n",
    "\n",
    "The alpha is the learning rate, and the sigmoid is the squashing activation function. The `*` multiplication is the normal element by element multiplication, and the `.` dot is the matrix dot product.\n",
    "\n",
    "The matrix of outputs from the previous layer is transposed, and this means the column of outputs becomes a row of outputs.\n",
    "\n",
    "In Python, "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# update the weights for the links between the hidden and output layers\n",
    "self.who += self.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the other weights between the input and hidden layers will be very similar. We just explot the symmetry and rewrite the code replacing the names so that they refer to the previous layers."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# update the weights for the links between the hidden and output layers\n",
    "self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Final Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "# neural network class definition\n",
    "class Neural_Network():\n",
    "    \n",
    "    # initialize the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        \n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrics, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node_i to node_j in the next layer\n",
    "        # w11, w21\n",
    "        # w12, w22 etc\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "    \n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "                    \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        #convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "\n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layers\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        # error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        \n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outpus))\n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "\n",
    "                    \n",
    "                    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        \n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "           \n",
    "        #calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)          \n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "                    \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
